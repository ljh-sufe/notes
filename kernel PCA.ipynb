{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "2022-3\n",
    "# Kernel PCA\n",
    "\n",
    "first recall normal PCA\n",
    "\n",
    "we have a sample matrix $X_{N,K}$, which means N samples and K dimension. However in the content of Kernel PCA, samples are vectors. And, basically vectors are column vectors. So, here we use $X_{K,N}$ instead.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "for PCA, first we have cov matrix $C=\\dfrac1N XX^T$, suppose that here $X$ is centered. Then we calculate the eigenvalue $\\lambda_i$ and eigenvector $u$ of $C$. After that we find $q$, the number of principle components we want to use. Let $U=[u_i]_{K,q}, i=1,2,...,q$, $X'=U^TX$ is the new matrix after dimensional reduction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "what if K >> N? Then the estimation of cov matrix is not precise, which may influence the performance of PCA. note that for any matrix we can do the SVD(Singular Value Decomposition)\n",
    "\n",
    "$$X=U\\Sigma V^T$$\n",
    "\n",
    "$U$ is a matrix where the columns are all eigenvectors of $XX^T$, so $U$ is a orthogonal matrix, hence $U^T=U^{-1}$.\n",
    "\n",
    "$$U^T X=\\Sigma V^T$$\n",
    "\n",
    "V = eigenvectors of $X^T X$ corresponding to the top q eigenvalues. Let $\\Sigma$ = diagonal matrix of square roots of the top q eigenvalues.\n",
    "$X'=\\Sigma V^T$ is the new matrix after dimensional reduction. Actually the method using SVD is called dual PCA."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "so basically we can say that PCA is a linear rotation of the orgional matrix. And the principle components are linearly independent. But what if the origional data has a nonlinear nature? If the greater the variable the better and the less the worse, then we can say this variable has a linear nature. That is to say this variable is linearly seperable. But what is the variable is nonlinearly seperable?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The answer is **Kernel PCA**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kernel PCA first mapping the data to a high dimensional space, then do the normal PCA. Let's do it step by step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "first, we use a nonlinear mapping $\\phi$ to map the origional matrix to a high dimension space $F$ (even infinite dimension). Note that we do not need to know what exactly $\\phi$ is. This is because we are going to use the kernel method!\n",
    "\n",
    "$$\\phi(X)=\\left[ \\phi(x_1), \\phi(x_2), ..., \\phi(x_N) \\right]_{D,N}$$\n",
    "\n",
    "then we have the covariance matrix $C_F=\\dfrac1N \\phi(X)\\phi(X)^T$, where $\\phi(X)$ is centralized. The same as in the normal PCA, next we want to find the eigenvalue and eigenvector of $C_F$\n",
    "\n",
    "$$C_F u=\\lambda u$$\n",
    "\n",
    "$$\\dfrac1N \\phi(X)\\phi(X)^T u = \\lambda u$$\n",
    "since the $\\frac1N$ does not influence the $u$, just a multiplier of $\\lambda$, we can remove it from the equation.\n",
    "\n",
    "$$\\phi(X)\\phi(X)^T u = \\lambda u \\tag{*}$$\n",
    "\n",
    "i.e.  $$\\sum_{i=1}^N \\phi(x_i)\\phi(x_i)^T u = \\lambda u$$\n",
    "$$u=\\dfrac1\\lambda \\sum_{i=1}^N \\phi(x_i)\\phi(x_i)^T u $$\n",
    "\n",
    "note that $\\phi(x_i)^T u $ is a scalar, we have\n",
    "\n",
    "$$u=\\sum_{i=1}^N \\alpha_i \\phi(x_i)$$\n",
    "\n",
    "where $\\alpha_i=\\dfrac{\\phi(x_i)u}{\\lambda} $. in the form of matrix, define $\\alpha=\\left[ \\alpha_1, \\alpha_2, ..., \\alpha_N \\right]^T$\n",
    "\n",
    "$$u=\\phi(X)\\alpha$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "plug $u$ into equation (*), \n",
    "$$\\phi(X)\\phi(X)^T \\phi(X)\\alpha = \\lambda \\phi(X)\\alpha $$\n",
    "\n",
    "multiply $\\phi(X)^T$ in both sides and define $K=\\phi(X)^T\\phi(X)$, $K$ is a $N$ dimension symmetric semidefinite matrix, $K_{ij}=\\phi(x_i)^T\\phi(x_j)$, we have \n",
    "$$\\phi(X)^T\\phi(X) \\phi(X)^T \\phi(X) \\alpha = \\phi(X)^T\\lambda \\phi(X)\\alpha $$\n",
    "\n",
    "$$KK\\alpha=\\lambda K \\alpha$$\n",
    "\n",
    "$$K\\alpha=\\lambda \\alpha$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "after we have $\\alpha$, the eignevector of $K$, we have diagonal matrix $\\Sigma$, where $\\Sigma_i=\\sqrt{\\lambda_i}$ and $V=\\left[ \\alpha_i \\right]$\n",
    "\n",
    "$$X'=\\Sigma V^T$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## some other dimension reduction methods\n",
    "\n",
    "by choosing different kernel $K$, we can have some other methods."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1 MDS\n",
    "\n",
    "$$K_{MDS} = -0.5(I-ee^T)D(I-ee^T)$$\n",
    "\n",
    "where $D$ is a distance matrix and $e=\\frac1N[1,1,...,1]^T$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given a distance matrix D, MDS attempts to find t data points y1, ..., yt in d dimensions, such that if $d_{ij}$ denotes the Euclidean distance between yi and yj , then $\\hat{D}$ is similar to $D$.\n",
    "\n",
    "$$min_Y \\sum_{i=1}^N \\sum_{j=1}^N (d_{ij}^{(X)} - d_{ij}^{(Y)})^2$$\n",
    "where $d_{ij}^{(X)}=||x_i-x_j||^2=(x_i-x_j)^T(x_i-x_j)$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "HD^{(X)}H &= (I-\\frac1N ee^T)D^{(X)}(I-\\frac1N ee^T) \\\\\n",
    "&= D^{(X)} - \\frac1Nee^TD^{(X)}-\\frac1ND^{(X)}ee^T+\\frac{1}{N^2}ee^TD^{(X)}ee^T \\\\\n",
    "&= D^{(X)} - \\frac1N \\tilde{D}^{(X)} - \\frac1N \\tilde{\\tilde{D}}^{(X)} + \\frac{1}{N^2}\\sum_{i=1}^N\\sum_{j=1}^N d_{ij} I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "so, the row $i$ column $j$ of $HD^{(X)}H$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& d_{ij}-\\frac1N \\sum_{p=1}^Nd_{pj}-\\frac1N \\sum_{p=1}^N d_{pi} + \\frac{1}{N^2} \\sum_{p=1}^N\\sum_{q=1}^N d_{pq} \\\\\n",
    "= & (x_i-x_j)^T(x_i-x_j) -\n",
    "\\frac1N\\sum_{p=1}^N [ (x_p-x_j)^T(x_p-x_j) \n",
    "+(x_p-x_i)^T(x_p-x_i)] + \\frac1{N^2}\\sum_{p=1}^N\\sum_{q=1}^N (x_p-x_q)^T(x_p-x_q)  \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "note that $X$ is centralized, $\\sum_{p=1}^Nx_p=0$, plug it in to the above equation we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "= & (x_i-x_j)^T(x_i-x_j) - \\frac1N \\sum_{p=1}^N [2x_p^Tx_p + x_j^Tx_j-x_i^Tx_i] + \\frac1{N^2}\\sum_{p=1}^N\\sum_{q=1}^N (x_p^Tx_p-x_q^Tx_p-x_p^Tx_q+x_q^Tx_q) \\\\\n",
    "= & (x_i-x_j)^T(x_i-x_j) - \\frac1N \\sum_{p=1}^N [2x_p^Tx_p + x_j^Tx_j-x_i^Tx_i] + \\frac1{N^2}\\sum_{p=1}^N\\sum_{q=1}^N 2x_p^Tx_p \\\\\n",
    "= & (x_i-x_j)^T(x_i-x_j) - \\frac1N N [ x_j^Tx_j-x_i^Tx_i] \\\\\n",
    "= & x_i^Tx_i - x_j^Tx_i - x_i^Tx_j + x_j^Tx_j - x_j^Tx_j-x_i^Tx_i \\\\\n",
    "= & - x_j^Tx_i - x_i^Tx_j \\\\\n",
    "= & -2 x_i^Tx_j \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "that is why $X^TX=-0.5HD^{(X)}H$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "here is a numerical proof"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(30, 9)\n",
    "X = (X.T - X.mean(axis=1)).T    # centralize X\n",
    "t, N = X.shape                  # t is the dimension and N is the sample size\n",
    "H = np.diag(np.ones(N)) - np.ones(N*N).reshape(N, N) / N\n",
    "D = np.ones(N*N).reshape(N, N)\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        D[i, j] = ((X[:, i]-X[:, j])**2).sum()\n",
    "A = -0.5*H.dot(D).dot(H)\n",
    "B = X.T.dot(X)\n",
    "(A-B)**2 < 1e-10                # A and B are the same!"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2 isomap\n",
    "\n",
    "$$K_{MDS} = -0.5(I-ee^T)D_g(I-ee^T)$$\n",
    "\n",
    "geodesic distance $D_g$\n",
    "\n",
    "How to find $D_g$?\n",
    "\n",
    "1. compute all the distances in euclidian space $D$.\n",
    "\n",
    "2. $\\tilde{d}_{ij}=d_{ij}*I\\{ d_{ij}<\\epsilon \\}$ ($\\epsilon$-isomap)  **or** $\\tilde{d}_{ij}=d_{ij}*I $ {i is one of the k nearest neighbours of j} (k-isomap)\n",
    "\n",
    "3. using Dijkstra's algorithm or Floyd-Warshall algorithm. input $[\\tilde{d_{ij}}]$, output $D_g$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3 SDE(semidefinite embedding)\n",
    "\n",
    "$K$ is learned from data\n",
    "\n",
    "$max$ $ trace(K)$\n",
    "\n",
    "s.t.   &nbsp;&nbsp; $K \\geq 0$ ($K$ is semidefinite)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\sum_{ij} K_{ij}=0$\n",
    "        \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\forall ij$, if $\\eta_{ij}>0$ or if $[\\eta^T \\eta]_{ij}>0$, then $K_{ii}-2K_{ij}+K_{jj}=||x_i-x_j||^2$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### biography\n",
    "\n",
    "http://www.math.uwaterloo.ca/~aghodsib/courses/f06stat890/readings/tutorial_stat890.pdf\n",
    "\n",
    "https://www.robots.ox.ac.uk/~az/lectures/ml/tenenbaum-isomap-Science2000.pdf"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b811c02d5585281d3c491c6bd7ab311f75186c4af269fa3e0e165a9b865fdff7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
