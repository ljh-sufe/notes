{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "maximize the distance between two lines,\n",
    "\n",
    "$l_{-1} : w^Tx+b=-1$\n",
    "$l_1 : w^Tx+b=1$\n",
    "\n",
    "suppose the distance is $r$, and a point on $l_{-1}$  is $ x_0$ , then the point $x_0 + r\\frac{w}{||w||}$ is on $l_1$, which means:\n",
    "$$\\begin{aligned}\n",
    "w^T(x_0+r \\frac{w}{||w||} )-b &=1 \\\\\n",
    "w^Tx_0 - b + r\\frac{w^Tw}{||w||}  &= 1 \\\\\n",
    "-1+r \\frac{||w||^2}{||w||}&= 1\n",
    "\\end{aligned}$$\n",
    "\n",
    "hence $r=\\frac{2}{||w||}$\n",
    "\n",
    "so maximize $r$ is equal to minimize $\\frac12 ||w||^2$\n",
    "\n",
    "also we have such constraints: if $y_i=1$, then the corresponding $x_i\\in \\{ x|w^Tx+b>1 \\}$ , if $y_i=-1$, the corresponding $x_i\\in \\{ x| w^Tx+b<-1\\}$.\n",
    "\n",
    "so the **primal problem** can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "min & \\quad \\frac12||w||^2 \\\\\n",
    "s.t. & \\quad y_i (w^Tx_i+b) \\geq 1\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "we have the lagrangian function\n",
    "$$L(x,\\lambda)=\\frac12||w||^2 + \\sum_i \\lambda_i(1-y_i (w^Tx_i+b))$$\n",
    "\n",
    "and\n",
    "$$\\Gamma(\\lambda)=\\underset{x}{inf}(\\frac12||w||^2 + \\sum_i \\lambda_i(1-y_i (w^Tx_i+b))) \\tag{*}$$\n",
    "\n",
    "first order condition for $\\Gamma$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w}=w^T+\\sum_i\\lambda_iy_ix_i=0$$\n",
    "\n",
    "and \n",
    "$$\\frac{\\partial L}{\\partial b}=\\sum_i \\lambda_i(-y_i)=0$$\n",
    "\n",
    "plug these results in to $(*)$, we have\n",
    "$$\\begin{aligned}\n",
    "\\Gamma&= \\underset{x}{inf} \\{ \\frac12 (w^Tw)+\\sum_i\\lambda_i(1-y_iw^Tx_i)-\\sum_i \\lambda_iy_ib\\} \\\\\n",
    "&= \\underset{x}{inf} \\{ \\frac12 w^Tw + \\sum_i\\lambda_i- w^Tw\\} \\\\\n",
    "&= \\underset{x}{inf} \\{ -\\frac12 w^Tw+\\sum_i\\lambda_i\\}\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "according to the lagrangian theorm, the **dual problem** is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "max & \\quad -\\frac12 w^Tw+\\sum_i\\lambda_i \\\\\n",
    "s.t. & \\quad \\lambda_i \\geq 0 \\\\\n",
    "& \\quad w = -\\sum_i \\lambda_i y_i x_i \\\\\n",
    "& \\quad \\sum_i \\lambda_iy_i=0\n",
    "\\end{aligned}$$\n",
    "\n",
    "since the primal problem is a convex problem, according to the **slater's condition,** there must exist a point $w^*$ in the feasible dimain, such that $y_i(w^{*T}x_i+b) > 1$. So, the strong suality holds and the duality gap is $0$. Hence, we can solve the dual problem and directly get the answer of the primal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### using the kernel method\n",
    "\n",
    "if the data are not linearly seperable in the original space $R^n$, we can do something similar to what we do in kernel PCA. that is to use a fernel function $\\Phi:R^n \\rightarrow R^h$, $h$ could goes to infinity. Then just by simply replace the $x$ in the original problem with $\\Phi(x)$\n",
    "\n",
    "the **dual problem** in the higher space is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "max & \\quad -\\frac12 w^Tw+\\sum_i\\lambda_i & ...... (1)\\\\\n",
    "s.t. & \\quad \\lambda_i \\geq 0 \\\\\n",
    "& \\quad w = -\\sum_i \\lambda_i y_i \\Phi(x_i) & ...... (3)\\\\\n",
    "& \\quad \\sum_i \\lambda_iy_i=0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "plug $(3)$ into $(1)$, the $w^Tw$ becomes\n",
    "\n",
    "$$\\begin{aligned}\n",
    "w^Tw &=(-\\sum_i \\lambda_i y_i \\Phi(x_i))^T \n",
    "(-\\sum_j \\lambda_j y_j \\Phi(x_j)) \\\\\n",
    "&= \\sum_i\\lambda_i  \\Phi^T(x_i) y_i^T \\sum_j \\lambda_j y_j \\Phi(x_j) \\\\\n",
    "&= \\sum_i \\sum_j \\lambda_i\\lambda_jy_i^Ty_j\\Phi^T(x_i)\\Phi(x_j)\n",
    "\\end{aligned}$$\n",
    "\n",
    "the dual problem becomes\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "max & \\quad -\\frac12 \\sum_i \\sum_j \\lambda_i\\lambda_jy_i^Ty_j\\Phi^T(x_i)\\Phi(x_j) +\\sum_i\\lambda_i \\\\\n",
    "s.t. & \\quad \\lambda_i \\geq 0 \\\\\n",
    "& \\quad \\sum_i \\lambda_iy_i=0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "by choosing a proper kernel function $k(i,j)=\\Phi^T(x_i)\\Phi(x_j) $ we can solve this in the Reproducing Kernel Hilbert Space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### soft margin SVM\n",
    "\n",
    "some times we do not need to find a \"perfect\" classifier, we can tolerate that the classifier makes some mistakes or misjudgments, which means for some samples $i$, \n",
    "\n",
    "$$y_i(w^T \\Phi(x_i)-b)<1$$\n",
    "\n",
    "so we can rewrite the primal problem as:\n",
    "\n",
    "$$\\begin{align*}\n",
    "min \\quad & \\frac12 w^Tw + \\beta \\sum_i \\textbf{I}\\{ 1-y_i(w^T\\Phi(x_i)-b) >0 \\}\n",
    "\\end{align*}$$\n",
    "\n",
    "where $I $ is an indicator function. However, this function is not convex and continuous, it may be not easy to solve this problem. We use the hinge loss and some variants instead\n",
    "\n",
    "$$min \\quad  \\frac12 w^Tw + \\beta \\sum_i \\max\\{0, 1-y_i(w^T\\Phi(x_i)-b)  \\}$$\n",
    "\n",
    "\n",
    "$$\n",
    "min \\quad  \\frac12 w^Tw + \\beta \\sum_i  \\left\\{ \n",
    "    \\begin{aligned} \n",
    "    & 0.5-y_i(w^T\\Phi(x_i)-b), & \\quad y_i(w^T\\Phi(x_i)-b)<0\\\\ \n",
    "    & 0.5(1-y_i(w^T\\Phi(x_i)-b))^2, & 0<y_i(w^T\\Phi(x_i)-b)<1 \\\\\n",
    "    & 0, & y_i(w^T\\Phi(x_i)-b)>1\n",
    "    \\end{aligned} \n",
    "\\right. \n",
    "$$\n",
    "(Rennie, Jason D. M.; Srebro, Nathan (2005). Loss Functions for Preference Levels: Regression with Discrete Ordered Labels. Proc. IJCAI Multidisciplinary Workshop on Advances in Preference Handling.)\n",
    "\n",
    "\n",
    "$$\n",
    "min \\quad  \\frac12 w^Tw + \\beta \\sum_i  \\left\\{ \n",
    "    \\begin{aligned} \n",
    "    & \\frac{1}{2\\gamma} max\\{0, 1-y_i(w^T\\Phi(x_i)-b)\\}^2, & \\quad y_i(w^T\\Phi(x_i)-b) \\geq 1-\\gamma\\\\ \n",
    "    & 1-\\frac{\\gamma}{2}-y_i(w^T\\Phi(x_i)-b), & y_i(w^T\\Phi(x_i)-b) < 1-\\gamma \n",
    "    \\end{aligned} \n",
    "\\right. \n",
    "$$\n",
    "(Zhang, Tong (2004). Solving large scale linear prediction problems using stochastic gradient descent algorithms. ICML.)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b811c02d5585281d3c491c6bd7ab311f75186c4af269fa3e0e165a9b865fdff7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
