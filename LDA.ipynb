
#### understanding latent dirichelet allocation.

to begin with, we need to understand dirichlet distribution. Actually, it's kind of like a distribution of parameters of another multinomial distribution. It measures the probability of probabilities.


the probability distribution of a multinomial distribution:

$$M(\vec{p}) = P(\vec{m}|n,\vec{p})=\frac{n!}{\prod{m_i}!}\prod p_i^{m_i} \propto \prod p_i^{m_i}$$

the probability dustribution of a dirichelet distribution:

$$D(\vec{\alpha})=P(\vec{p}| \vec{\alpha} ) = \dfrac{\Gamma(\sum_{k=1}^K a_k)}{\prod_{k=1}^K \Gamma(a_k)} \prod p_k^{\alpha_k-1}  \propto \prod p_k^{\alpha_k-1}$$

so in bayes analysis, we use dirichlet distribution as the prior distribution, and use multinomial distribution to calculate the likelihood. Hence the posterior distribution is still a dirichlet distribution.

$$
\begin{aligned}
P(\vec{p} | n,\vec{m},\vec{\alpha}) & \propto  P(\vec{m}|n,\vec{p},\vec{a}) \times P(\vec{p}|\vec{\alpha}) \\
&= \prod p_i^{m_i} \prod p_k^{\alpha_k-1} \\
&= \prod p_i^{m_i + \alpha_i-1}
\end{aligned}$$

we can denote this process as 

$$D(\vec{\alpha}) + M(\vec{p}) = D(\vec{\alpha} + \vec{m})$$


<br>
<br>
<br>

In LDA, we assume there are K topics and M documents. There are $N_d$ words in the $d^{th}$ documents.

For a certain document $d$, we use a dirichlet distribution to fit the probability of $K$ topics. $\theta_d$ is a K-dimensional vector.

$$\theta_d \sim D(\vec{\alpha})$$

For a certain topic $k$, we also use a dirichlet distribution to fit the distribution of words belongs to this topic. $\beta_k$ is a V-dimensional vector, V is the number of words in the dictionary.

$$\beta_k \sim D(\vec{\eta})$$

for $n^{th}$ word in document $d$, it could belongs to topic $1,2,...,K$,

$$Z_{dn} \sim M(\theta_d)$$

also for a certain topic $Z_{dn}$, the probability to get word $w_{dn}$ is

$$w_{dn} \sim M(\beta_{Z_{dn}})$$


now suppose in document $d$, there are $n_d^{(k)}$ words belongs to topic $k$,

$$\vec{n_d} = ( n_d^{(1)}, n_d^{(2)}, ..., n_d^{(K)}  )$$

then we can update the posterior of $\theta_d \sim D(\vec{\alpha} + \vec{n_d} )$

in topic $k$, the number of the $v^{th}$ word is $n_k^{(v)}$,

$$\vec{n_k} = (n_k^{(1)}, n_k^{(2)},  ..., n_k^{(V)})$$

hen we can update the posterior of $\beta_k \sim D(\vec{\eta} + \vec{n_k} )$

<br>
<br>
<br>

let us summarize the LDA Gibbs sampling algorithm flow.  first is the training process:

1) Choose the appropriate number of topics $K$, choose the appropriate hyperparameter vector $\vec{\alpha}$, $\vec{\eta}$

2) Corresponding to each word of each document in the corpus, randomly assign a topic category $z$

3) Rescan the corpus. For each word, use the Gibbs sampling formula to update its topic category, and update the number of the word in the corpus.

4) Repeat the Gibbs sampling based on axis rotation in step 2 until the Gibbs sampling converges.

5) Count the topics of each word in each document in the corpus to obtain the document topic distribution $\theta_d$, and count the distribution of each topic word in the corpus to obtain the LDA topic and word distribution $\beta_k$ 

now the word distribution $\beta_k$ is set. When a new article arrives, we only need to update the $\theta_d$ part.
